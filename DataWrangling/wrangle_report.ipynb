{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Wrangling - @dog_rates tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this exercise we took data from multiple sources related to a series of tweets from the user account of @dog_rates from the twitter platform. This account generates short tweets and images of dog a long with a rating. This reating is typically out of 10, with the majority being greater than 10 out of 10.\n",
    "\n",
    "As mentioned the data comes from multiple sources outlined as follows\n",
    "\n",
    "1. `twitter_archive_enhanced.csv` is an archive of @dog_rates tweets. This file contains a limited amount of information about the tweet.\n",
    "2. `image_predictions.tsv` is an image prediction data set using the @dog_rates tweeted images with a prediction of the dog breed in the picture. 3 predictions are included and a number of them are incorrectly predicting non-dogs for the images.\n",
    "3. `tweet_json.txt` is a text file containing a json dump that was extracted from the twitter api. This dataset has additional supplemental data about the archived @dog_rates tweets.\n",
    "\n",
    "Here I will briefly outline the steps taken in gathering, tidying and cleaning these datasets so we are left with one single dataset which can be used for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering\n",
    "\n",
    "To gather the data we used 3 different methods - 1. manually navigating and downloading 2. pythons request module 3. python's tweepy module to communicate with twitter's api.\n",
    "\n",
    "First we downloaded the archived data file, `twitter_archive_enhaanced.csv`, manually from Udacity. This file was saved down to our local machine. The link to download can be found <a href=\"https://video.udacity-data.com/topher/2018/November/5bf60c1e_twitter-archive-enhanced-2/twitter-archive-enhanced-2.csv\">here</a>.\n",
    "\n",
    "Second we used the requests module to download the `image_predictions.tsv` and written locally to file.\n",
    "\n",
    "Finally we utilized the tweepy module to pull in each of the tweets that is included in the `twitter_archive_enhanced.csv` file. Due to the limit of tweets we can query using the api we broke this into chunks of 100 tweets. We appended this into a list and then ultimately dump the json text into a file named `tweet_json.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness\n",
    "\n",
    "To get one single file for analysis we first focused on joining the additional tweet data from `tweet_json.txt` to that of `twitter_archive_enhanced.csv`. In our analysis we will only worry about the retweet and favorite count for each of the tweets. We utilized python's pandas module to do our data joins.\n",
    "\n",
    "Before joining the image data we first dropped a number of the tweet columns in our dataset we do not want. We drop the retweet and reply columns as we won't be including data of those types of tweets.\n",
    "\n",
    "After enhancing with the additional tweet data attention was turned to joining in the `image_predictions.tsv` data. We want to end up with only highest predicted dog breed. This means we ultimately do not include predictions that are not dog breeds. To do this we flatted the dataset so that instead of all 3 predictions there is only 1 single prediction, the highest breed dog predicted, and the pred_confidence of that single prediction. Then we took that data and joined the 4 columns to the archive dataset.\n",
    "\n",
    "Now we had a dataset that had both additional tweet data and image prediction data which was now a tidy file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanliness\n",
    "\n",
    "Using programmatic and manual techniques we assessed the quality of our data. This mostly consisted of utilizing pandas built in techniques to observe data types and value counts. Based on this information we observed 8 issues that needed to be resolved.\n",
    "\n",
    "   - Erroneous datatype for timestamp\n",
    "   - Multiple columns for dog state (doggo, floofer, pupper, puppo)\n",
    "   - missing records for expanded_urls\n",
    "   - missing records for retweet_count and favorite_count\n",
    "   - rating_numerator has values less than 10\n",
    "   - rating_denominator has values not equal to 10\n",
    "   - source column has unnecessary text\n",
    "   - set all predicted dog values to lower case strings\n",
    "   - remove retweets/replies\n",
    "\n",
    "Some of these cases were easy to fix, changing data types and setting predictions to all lower case, but other were more involved, fixing the rating columns. Ultimately we dropped missing records along with adjusting the ratings so that there ended up being a single column which was the numerator all updated so they were out of a rating of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In conclusion we have ended up with a single file, `twitter_archive_master.csv`, which is in a tidy state with quality issues address. Likely through use of this file we will observe additional quality issues that have yet to be cleaned up and this should be updated as such issues are observed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wranlingproject]",
   "language": "python",
   "name": "conda-env-wranlingproject-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
